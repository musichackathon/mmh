---
layout: post
title: Hacks from the 2014 Back-To-School Hackathon
date: '2015-09-19T20:59:00-04:00'
tags:
- music
- education
- hackathon
- '2014'
- new york
tumblr_url: http://monthlymusichackathon.org/post/129456162522/music-ed-hack-2014-review
---
<p>Last September, Monthly Music Hackathon put on the <a href="http://monthlymusichackathon.org/2017-07-04-2014-08-17-school/">Back To School Hackathon</a>. We had 12 speakers from several NYC-area music and music tech university programs give overviews of their programs, or specific projects they were working on. Then we had a full day to collaboratively hack on the ideas discussed.</p><p>It was really fun, so this year we&rsquo;re doing a similar event, the <a href="http://monthlymusichackathon.org/2017-07-04-2015-09-06-education/">Music Education Hackathon</a> coming up next week. The goal this year is to deeply explore the potential of music and music tech education, challenges faced by students and students, and creative solutions to those challenges.</p><p>Below is a re-posting of a comprehensive review of all the talks and hack presentations at last year&rsquo;s Back To School Hackathon, written by Matt Spry in September 2014.<br/></p><p>Please join us at the 2015 Music Education Hackathon at Spotify’s NYC offices. Talks are Friday, September 25th, 2015 at 8pm. The Hackathon is from Noon to 8pm Saturday, September 26th, 2015. Hackers’ demos of their work will be at 8pm on Saturday, September 26th. All events are free and all are welcome.</p><p><br/></p><hr><h2><br/></h2><h2>Class is in Session at Back-to-School Hackathon</h2><p><i>Matt Spry, September 2014</i></p><p>The academic year is in full-swing. New backpacks and messenger bags are broken in, and the sweat of clumsy dormitory move-ins has been wiped away. For a few hundred engineers, students, educators, and musicians who gathered at Spotify’s New York City office last month, the start of the “back to school” season was marked by a ragtime performance from a man in a skunk costume playing the xylophone and a bass player dressed as a pink gorilla. On the subway platform at 14th Street, this might qualify as a busking spectacle; at September’s “back to school” edition of <a href="http://monthlymusichackathon.org/">Monthly Music Hackathon NYC</a>, it was the opening act.</p><p>This unique performance by the <a href="http://www.xylopholks.com/">Xylopholks</a> was followed by roughly a dozen short-form talks from educators, researchers, and students from the NYC area who discussed a mix of both personal work and ongoing projects in their departments and organizations.</p><h2>About the Talks</h2><p>Colin Raffel, a PhD  candidate at Columbia University, discussed <a href="http://labrosa.ee.columbia.edu/">LabROSA</a> (The Laboratory for Recognition and Organization of Speech and Audio) and provided a thorough overview of the Lab’s research and current projects.</p><p>Brian McFee, current fellow at NYU’s <a href="http://cds.nyu.edu/">Center for Data Science</a>, discussed his study of machine learning and musical analysis. He also described the <a href="https://github.com/bmcfee/librosa/">LibROSA</a> tool, which he started at a Monthly Music Hackathon two years ago and is used for harmonic percussive separation and spectrograms, among other signal-based music analysis.</p><p>Jascha Narveson is a composition instructor at the <a href="http://www.newschool.edu/mannes/">Mannes College of Music</a> at the New School and a member of <a href="http://sidebandband.com/">Sideband</a>, an offshoot of the Princeton Laptop Orchestra. He wrote <a href="http://jaschanarveson.com/pages/code.html">LANdini</a>, a networking software for Mac OS 10.6+, to improve OSC communications during their wireless performances. </p><p>Hans Tammen, deputy director of the not-for-profit organization <a href="http://www.harvestworks.org/">Harvestworks</a>, provided an overview of its Technology, Engineering, Art and Music (TEAM) Lab and the organization’s guiding principle of the democratization of technology.</p><p>Eric Humphrey, a PhD candidate in the Music Informatics group at NYU’s Music and Audio Research Laboratory (<a href="http://marl.smusic.nyu.edu/">MARL</a>), discussed his desire to create audio analysis tools for “15 year old versions of [himself],” and briefly touched upon an algorithm-in-process to create a Guitar Hero-like experience for one’s favorite songs in Spotify.</p><p>Jason Sigal, a grad student at NYU’s <a href="http://itp.nyu.edu/itp/">Interactive Telecommunications Program</a>, provided an overview of past and current projects at ITP, including the <a href="http://itp.nyu.edu/shows/spring2013/the-well-sequenced-synthesizer/">well sequenced synthesizer</a> (physical interfaces that control simple generative algorithms) and <a href="http://itp.nyu.edu/shows/winter2012/firewall/">firewall</a> (a sheet of spandex that senses when it’s being stretched and based on that, generates different visuals and music). Sigal has also contributed an add-on audio library to the newest version of <a href="http://p5js.org/">p5.js</a>, itself based on the <a href="https://processing.org/">Processing</a> programming language.</p><p>Justin Salomon, a post-doctoral researcher in the Music Informatics group at NYU’s MARL, discussed his work on a melody extraction plugin for Sonic Visualizer called <a href="http://www.justinsalamon.com/melody-extraction">MELODIA</a>, which attempts to extract the series of pitches that correspond to the main melody in a given song.</p><p>Alex Ruthmann, associate professor of music education and music technology at NYU, discussed the recently formed Music Experience Design Lab (<a href="http://www.musedlab.org/">MusEDLab</a>), which collaborates with industry and community partners to design and develop new experiences for music making, engagement, and learning.</p><p>Uri Nieto, a PhD candidate in music technology at MARL at NYU, discussed audio signal processing with a specific focus on music structure analysis and his ongoing work on Motives Extractor, which uses music segmentation to identify polyphonic musical patterns in a given song.</p><p>Peter Gordon, head of music technology at <a href="http://www.bloomfield.edu/">Bloomfield College</a>, a founding member of Love of Life Orchestra, and a self-professed “knobs and dials guy,” discussed his work as a composer and various ongoing multimedia collaborations.</p><p>Avneesh Sarwate, a software engineer and recent graduate of Princeton University’s computer science program, provided an overview of Skip Step, a touchscreen application that lets the user write music, create loops, and edit in real time. Each element in the grid interface can be played like a piano with chord-to-single-finger mappings, and you can switch between four different midi channels at once.</p><p>Tae Hong Park, associate professor of music technology and composition at NYU, placed a major emphasis on accessibility and low cost when innovating new tools. His fortissimo (ff) project added tactile force feedback to mobile computing devices and touchscreen interfaces with no real hardware modification and utilization of the device’s on-board features (e.g. sensors, accelerometers). <a href="http://citygram.smusic.nyu.edu/">Citygram</a>, started in 2011, seeks to provide real-time, data-driven visualizations of sound and noise patterns in urban environments.</p><h2>About the Hacks</h2><p>The demonstration portion of the hackathon included a wide variety of final products, from functioning prototypes to conceptual designs. Here’s a list of descriptions of the projects and personnel in the order in which they were presented: </p><p>Colin Raffel added <a href="https://gist.github.com/craffel/3eb7513d4540f4acee93">some functionality</a> to a Python module called <a href="https://github.com/craffel/pretty-midi">pretty-midi</a>, including functions to synthesize drum instruments and arpeggiate notes, and a separate wrapper function which synthesizes instruments according to type (as either a drum, a bass triangle wave, or an arpeggiated square wave). The result of this work essentially turns midi data into chiptune-like audio files.</p><p><a href="https://github.com/BrandonDGr8/TempoPerfect/blob/master/TempoPerfect.html">TempoPerfect</a>, a hack from Yijin Kang, Brandon Yu, Hugo Melo, Anton Sterlin, Frederica Chen, is a game that trains the user to be more on time with the beat of a given song via his or her keyboard, inclusive of feedback and a scoring system. While technical difficulties prevented the group from fine-tuning its features in time for the presentation, the audience was treated to some impromptu beat-boxing and freestyling from two of their members.</p><p>Students from the Bronx High School of Science, Martin Awano and Will Yarinsky, harvested authentic sounds in the Spotify office during the hacking period &ndash; voices, crushing chips, etc. &ndash; to create a soundscape they called “Sounds of Spotify.” </p><p>Michelle Chu, Alex Dorsk, Pat Harry, Roy Livne, Perry Sherman, and Muhammed Ziauddin, conceptualized an app called <a href="http://coach-eoke.github.io/coach-eoke/">Coach-eoke</a> that would find the bottom and top of a user’s vocal range and provide achievable karaoke songs for that person to sing. The common karaoke-goer often selects songs based on personal familiarity instead of his or her natural ability, and this app would mitigate that issue. Future steps included additional mock-ups, a functional prototype, training features, and testing.</p><p>Fresh off her completion of the <a href="http://girlswhocode.com/programs/">Girls Who Code</a> summer immersion program, Victoria Gnip produced Moodplay, a website that provides a playlist of hand-selected songs from Spotify based on the user’s self-identified mood. Her future steps included expanding the current list of selectable moods beyond the current six options, and looking at acoustical attributes of songs to determine the mood category instead of hand-picking them.</p><p>We all love Soundcloud because of its unique content, but Jonathan Kupferman is one of many users who finds it difficult to discover new songs related to the ones he loves. Heartcloud, a hack that uses collaborative filtering to find similar songs, is one potential solution to this issue. Instead of using tags or audio analysis, Jonathan’s tool makes around 1,000 API requests to Soundcloud in order to find similar songs based on the tracks other users liked.</p><p>Finn Upham, a PhD student at NYU researching music cognition, produced a template experiment interface in PureData that plays an audio recording (i.e. stimulus), records the user’s self-reported engagement and pleasure levels, and repeats the process a number of times before generating a text file of the results. The recordings she used for the demo included electronic music, a snippet of bluegrass, and evocative spoken word poetry.</p><p>Want to practice your singing but don’t have a piano handy? <a href="https://github.com/VjiaoBlack/LeapAudio">LeapChords</a>, a project from Victor Jiao, Mohammed Ali, and Grace Zhang, seeks to make it easier to practice singing with a background chord playing. The app utilizes <a href="https://www.leapmotion.com/">LeapMotion</a> to allow the user to play chords using only finger and hand gestures. One can also control the volume of the chords by moving one’s hands closer to or farther from the device.</p><p>Uri Nieto provided an extension of the work he presented earlier in the day (see above), using Colin Raffel’s pretty-midi module to sonify and concatenate the patterns his Motives Extractor algorithm extracted from a piece of classical piano music. Uri’s enhancements also provide the exact time points at which all of these patterns occurred in the original piece.</p><p>The <a href="https://github.com/zopf/p5-elm-audio">Spinny Beats</a> project from Alec Zopf and Carlos De La Guardia was one of a few compelling approaches to visualization of audio. They chose to code in <a href="http://elm-lang.org/">Elm</a> because it was more concise than some other languages and also allowed them to express UI features more easily. Their end result was a bouncing ball on the screen that changed in color and size based on the user’s microphone input (in this case, some beatboxing).</p><p>After providing an overview of a few ongoing projects &ndash; <a href="http://worldup.org/blog/?page_id=2">World Up</a> and <a href="http://thelivingremix.bandcamp.com">The Living Remix</a> among them &ndash; Aaron Lazansky-Olivas (aka <a href="https://twitter.com/spazecraft">Spazecraft One</a>) turned things over to Brandon Bennett (<a href="https://www.facebook.com/pages/I-am-B13/162714883761716">I Am B13</a>), a student at College of Staten Island. Bennett played an original hip-hop instrumental he composed during the day that was inspired by the work of J Dilla.</p><p>In one of the few hardware hacks of the day, <a href="http://adamnovember.com/">Adam November</a> demonstrated <a href="https://www.youtube.com/watch?v=2r_GGvuMwr0">Groove Pizza</a>, a physical “drum machine” where you can add and remove beats by placing pennies on different parts of a cardboard circle. Placing the pennies on the “pizza” connects metal rings to wires connected to different pins; this physical contact gets picked up by a computer and translated as one of eight potential beat sequences. Separate knobs on the device control tempo and swing.</p><p>A number of participants from <a href="http://www.allstarcode.org/">All Star Code</a> &ndash; Mohammed, Ashim, Austin, Gary, Devon, Mamadou, and Djassi &ndash; collaborated on Music Matcher, a web-based memory game which involves matching separate Spotify tracks based on snippets of audio. Correct matches are wiped from the screen, leaving only the unmatched tracks. They solicited choices from the audience (some correct, some totally wrong) during a lively demo of the game.</p><p>Brian McFee then demonstrated a tool that utilized spectral clustering to extract fragments of melodies from audio recordings. While he claimed that it didn’t work the way he’d hoped, the Frank Sinatra sample he played for the audience conveyed the intent of the hack.</p><p>As another example of how these monthly hackathons can help facilitate meaningful continuity, Jason Sigal and Karen Feng presented an updated version of their <a href="http://karenpeng.github.io/yayaya/">yayaya</a> tool, which uses your computer’s camera and creates colorful triangular shapes and arpeggios based on motion detection in your web browser. Incorporating suggestions from T.K. Broderick, they enhanced the sustainability of the on-screen shapes, used arpeggios based on the sizes of the shapes, and also used the JavaScript libary <a href="https://github.com/TONEnoTONE/Tone.js/">Tone.js</a> developed by Yotam Mann. Future plans include potentially building additional functionality to map the notes in the arpeggio to different facial expression categories, such as happiness, sadness, anger, and surprise.</p><p>Participants who didn’t present a hack or project still felt that there was tremendous value in attending the event. <a href="http://www.lilyvirginia.com/">Lily Virginia</a>, a Brooklyn-based musician, used the hackathon as a networking opportunity and a sounding board for a prospective Kickstarter campaign she had been conceptualizing. A few groups added incremental improvements to ongoing projects hatched at previous hackathons. Another group was working on a new app/VST that would trigger real drumset samples based on sounds a user makes while beatboxing into a microphone. One experienced JavaScript programmer to whom we spoke used the afternoon to learn the <a href="http://clojure.org/">Clojure</a> syntax and familiarize himself with <a href="http://overtone.github.io/">Overtone</a>. The personal learning project was challenging, and taught him that what he “thought was programming was just one style of programming,” which made him think more critically about how to better teach the building blocks for performing basic tasks to others.</p>
